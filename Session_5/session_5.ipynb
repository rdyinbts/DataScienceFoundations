{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "### [Tokenization](http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html) is the process of chopping a sequence of characters into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation.\n",
    "\n",
    "### Tokenizers are commonly implemented in off-the-shelf softwares, such as [NLTK](http://www.nltk.org/) and [Stanford CoreNLP](http://stanfordnlp.github.io/CoreNLP/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import word tokenizer from NLTK\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = 'Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services. Its hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, and the Apple Watch smartwatch.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'Inc.', 'is', 'an', 'American', 'multinational', 'technology', 'company', 'headquartered', 'in', 'Cupertino', ',', 'California', ',', 'that', 'designs', ',', 'develops', ',', 'and', 'sells', 'consumer', 'electronics', ',', 'computer', 'software', ',', 'and', 'online', 'services', '.', 'Its', 'hardware', 'products', 'include', 'the', 'iPhone', 'smartphone', ',', 'the', 'iPad', 'tablet', 'computer', ',', 'the', 'Mac', 'personal', 'computer', ',', 'the', 'iPod', 'portable', 'media', 'player', ',', 'and', 'the', 'Apple', 'Watch', 'smartwatch', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "text_tokenized = word_tokenize(text)\n",
    "print(text_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "### Lemmatization / stemming is the process of reducing the inflectional forms or derivationally related forms of a word to a common base form. Lemmatization / stemming is useful in reducing redundant information and computational cost. It used to be popular at the time when computational resources is limited. However, stemming also increases the risk of losing information by ignoring the difference between words of different inflectional forms. Nowadays, as computational resources become much cheaper, researcher are using stemming less than before.\n",
    " \n",
    "### This section demonstrated stemming using the widely-used [Porter Stemmer](http://tartarus.org/martin/PorterStemmer/).\n",
    "### The Porter stemmer is a process for removing the commoner morphological and inflextional endings from words in English. Its main use is as part of a term normalization process that is usually done when setting up Information Retrieval systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appl', 'inc.', 'is', 'an', 'american', 'multin', 'technolog', 'compani', 'headquart', 'in', 'cupertino', ',', 'california', ',', 'that', 'design', ',', 'develop', ',', 'and', 'sell', 'consum', 'electron', ',', 'comput', 'softwar', ',', 'and', 'onlin', 'servic', '.', 'it', 'hardwar', 'product', 'includ', 'the', 'iphon', 'smartphon', ',', 'the', 'ipad', 'tablet', 'comput', ',', 'the', 'mac', 'person', 'comput', ',', 'the', 'ipod', 'portabl', 'media', 'player', ',', 'and', 'the', 'appl', 'watch', 'smartwatch', '.']\n"
     ]
    }
   ],
   "source": [
    "# Construct a Porter Stemmer instance\n",
    "porter = nltk.PorterStemmer()\n",
    "# Loop over the tokenized text to stem words\n",
    "text_stemmed = [porter.stem(t) for t in text_tokenized]\n",
    "print(text_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Removal\n",
    "### Stopwords is a set of most common words that are believed to carry little information, and thus are removed from the documents in the preprocessing step. There is actually no unique stopwords list, but some software such NLTK provides a set of most common functional words in English, and researchers can build their own list of stopwords based on their domain knowledge. Removing stopwords is believed to reduce the computational cost and reduce the noises in the text data. However, it is not a necessary step in every textual analysis project because it also increase the risk of losing useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "# Show the English stopwords list in NLTK\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'Inc.', 'American', 'multinational', 'technology', 'company', 'headquartered', 'Cupertino', ',', 'California', ',', 'designs', ',', 'develops', ',', 'sells', 'consumer', 'electronics', ',', 'computer', 'software', ',', 'online', 'services', '.', 'Its', 'hardware', 'products', 'include', 'iPhone', 'smartphone', ',', 'iPad', 'tablet', 'computer', ',', 'Mac', 'personal', 'computer', ',', 'iPod', 'portable', 'media', 'player', ',', 'Apple', 'Watch', 'smartwatch', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords from the tokenized text\n",
    "text_swremoved = [w for w in text_tokenized if w not in stopwords.words('english')]\n",
    "print(text_swremoved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech tagging\n",
    "### Part-of-speech (POS) tagging is the process of marking up a word as a particular part of speech such as nouns, verbs, adjectives, etc., based on its grammartical relationship with other words in the text. Part-of-speech tagging is useful in distinguish the words of the same forms but may have different part-of-speech tags, and the part-of-speech tags can be used in downstream NLP applications such as phrase generating. \n",
    "\n",
    "### The table of part-of-speech tags is available [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html \"Penn Treebank Part-of-Speech Tags\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'NNP'), ('Inc.', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('American', 'JJ'), ('multinational', 'NN'), ('technology', 'NN'), ('company', 'NN'), ('headquartered', 'VBD'), ('in', 'IN'), ('Cupertino', 'NNP'), (',', ','), ('California', 'NNP'), (',', ','), ('that', 'WDT'), ('designs', 'VBZ'), (',', ','), ('develops', 'NNS'), (',', ','), ('and', 'CC'), ('sells', 'VBZ'), ('consumer', 'NN'), ('electronics', 'NNS'), (',', ','), ('computer', 'NN'), ('software', 'NN'), (',', ','), ('and', 'CC'), ('online', 'NN'), ('services', 'NNS'), ('.', '.'), ('Its', 'PRP$'), ('hardware', 'NN'), ('products', 'NNS'), ('include', 'VBP'), ('the', 'DT'), ('iPhone', 'NN'), ('smartphone', 'NN'), (',', ','), ('the', 'DT'), ('iPad', 'NN'), ('tablet', 'NN'), ('computer', 'NN'), (',', ','), ('the', 'DT'), ('Mac', 'NNP'), ('personal', 'JJ'), ('computer', 'NN'), (',', ','), ('the', 'DT'), ('iPod', 'NN'), ('portable', 'JJ'), ('media', 'NNS'), ('player', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('Apple', 'NNP'), ('Watch', 'NNP'), ('smartwatch', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Import the NLTK pos tagger\n",
    "from nltk import pos_tag\n",
    "# Generate Part-of-Speech tags\n",
    "text_postagged = pos_tag(text_tokenized)\n",
    "print(text_postagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entity Recognition\n",
    "### Named entity recognition is the process of identifying and classifying elements in text into pre-defined categories such as the names of persons, organizations, locations and so on. Named entity recognition can be useful in semantic analysis, and some applications in social science including build organizational network using cross references through name entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Apple/NNP)\n",
      "  (ORGANIZATION Inc./NNP)\n",
      "  is/VBZ\n",
      "  an/DT\n",
      "  (GPE American/JJ)\n",
      "  multinational/NN\n",
      "  technology/NN\n",
      "  company/NN\n",
      "  headquartered/VBD\n",
      "  in/IN\n",
      "  (GPE Cupertino/NNP)\n",
      "  ,/,\n",
      "  (GPE California/NNP)\n",
      "  ,/,\n",
      "  that/WDT\n",
      "  designs/VBZ\n",
      "  ,/,\n",
      "  develops/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  sells/VBZ\n",
      "  consumer/NN\n",
      "  electronics/NNS\n",
      "  ,/,\n",
      "  computer/NN\n",
      "  software/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  online/NN\n",
      "  services/NNS\n",
      "  ./.\n",
      "  Its/PRP$\n",
      "  hardware/NN\n",
      "  products/NNS\n",
      "  include/VBP\n",
      "  the/DT\n",
      "  (ORGANIZATION iPhone/NN)\n",
      "  smartphone/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (ORGANIZATION iPad/NN)\n",
      "  tablet/NN\n",
      "  computer/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (ORGANIZATION Mac/NNP)\n",
      "  personal/JJ\n",
      "  computer/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (ORGANIZATION iPod/NN)\n",
      "  portable/JJ\n",
      "  media/NNS\n",
      "  player/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  the/DT\n",
      "  (ORGANIZATION Apple/NNP Watch/NNP)\n",
      "  smartwatch/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Import the name entity tagger from NLTK\n",
    "from nltk import ne_chunk\n",
    "# Generates named entity tags\n",
    "text_nertagged = ne_chunk(text_postagged)\n",
    "print(text_nertagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Split\n",
    "### Sentence splitting is the process of dividing a continuous document into separate sentences by identifying the boundaries of the sentences. It can be useful in word embedding models, which will be covered in the last lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services.\n",
      "-----\n",
      "Its hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, and the Apple Watch smartwatch.\n"
     ]
    }
   ],
   "source": [
    "# Import the sentence splitter from NLTK\n",
    "import nltk.data\n",
    "sent_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# Split the paragraph into sentences\n",
    "print('\\n-----\\n'.join(sent_splitter.tokenize(text.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services.\n",
      "Its hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, and the Apple Watch smartwatch.\n"
     ]
    }
   ],
   "source": [
    "for el in sent_splitter.tokenize(text.strip()):\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vector Presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words\n",
    "### Bag-of-words is one of the simplest language models in textual analysis. The idea is to map each document in a vector space. The dimension of the vector space is the size of the vocabulary, with each dimension representing a unique word in the dictionary. The values of the word vector represent weights assigned to the words. The underlying assumption of the bag-of-words model is that only that numbers that a word appear in a document matters, but the context information or the position information is not useful at all. This is a very strong assumption. However, the bag-of-words model is still one of the most popular models in computational linguistics because it often outperforms most sophisticated language models when training using large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'turmoil', 'endemic', 'differs', 'hard', 'problem', 'last', 'found', 'Economy', 'Obama', 'always', 'What', '?', 'a', 'people', 'At', 'sanctions', 'view', 'economy', 'Japan', 'domestic', 'Abe', 'has', 'horse', 'daily', 'accelerating', 'serious', 'towards', 'future', ':', 'own', 'the', 'was', '-', 'Ruble', 'Shinzo', 'again', 'growing', 'Abenomics', 'at', 'while', 'confronting', 'pace', 'even', 'eased', 'value', 'China', 'deer', 'rapid', 'for', 'We', \"'s\", 'his', 'US', 'asked', 'shirt', 'be', 'in', 'violence', 'things', 'and', 'falls', 'corruption', 'Vladimir', 'minister', 'views', 'Cuba', 'fix', 'is', 'country', 'it', 'economic', 'healing', 'working', 'Russian', 'to', 'However', 'seems', 'riding', 'so', 'on', 'horses', 'from', 'strong', 'without', ',', 'that', 'of', 'almost', 'an', 'those', 'prime', '.', 'politically', 'about', 'hunting', 'Russia', 'as', 'against', 'greatly', 'Putin', 'tumbled'}\n"
     ]
    }
   ],
   "source": [
    "document_0 = \"China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\"\n",
    "document_1 = \"At last, China seems serious about confronting an endemic problem: domestic violence and corruption.\"\n",
    "document_2 = \"Japan's prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.\"\n",
    "document_3 = \"Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.\"\n",
    "document_4 = \"What's the future of Abenomics? We asked Shinzo Abe for his views\"\n",
    "document_5 = \"Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble's value falls almost daily.\"\n",
    "document_6 = \"Vladimir Putin was found to be riding a horse, again, without a shirt on while hunting deer. Vladimir Putin always seems so serious about things - even riding horses.\"\n",
    "\n",
    "all_documents = [document_0, document_1, document_2, document_3, document_4, document_5, document_6]\n",
    "tokenized_documents = [word_tokenize(d) for d in all_documents] # tokenized docs\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "print(all_tokens_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF, or Term Frequency, measures one thing: the count of words in a document. It's the first step for TF-IDF or Term Frequency Inverse Document Frequency.\n",
    "\n",
    "### First comes the easy part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above we've created a simple way of counting the number of occurrences of a token in a document. There are plenty of ways to do this, through collections or for loops but they all end with the same result, how many times does a token appear in a document.\n",
    "\n",
    "### However this does not resolve one issue - this is biased towards longer documents. To remedy this we should weight terms according to document length or normalize them on another scale. This process is called normalization. While there are several ways to do this, I'll only show two, sub-linear term frequency and augmented frequency measurement. Here's the formula:\n",
    "<img src=\"tf.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    return 1 + math.log(tokenized_document.count(term))\n",
    "\n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These two different normalization techniques obviously approach our goal differently. We'll be focusing on the augmented term frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next part of TF-IDF is the IDF or inverse document frequency. Basically we want to target the words that are unique to certain documents instead of those that appear in all the documents because by definition, those are not good identifiers for any given document. Here's our equation for IDF.\n",
    "<img src=idf.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we've got a weight for every token in every document. This helps determine the important words from the unimportant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9459101490553135\n"
     ]
    }
   ],
   "source": [
    "idf_values = inverse_document_frequencies(tokenized_documents)\n",
    "print(idf_values['Abenomics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.336472236621213\n"
     ]
    }
   ],
   "source": [
    "print(idf_values['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we see the power of TF-IDF. We've taken the inverse of a token's document frequency. This means that words that appear a lot like \"the\" are weighted much less than words like \"Abenomics\". However TF-IDF allows us to do so much more. We can create vector representations of sentences or more simply, we can transform documents into numbers and lists of documents into matrices.\n",
    "\n",
    "### While this seems relatively unimportant, it actually allows us to do a lot of exceptional things. We have a mathematical representation of a sentence and we can now use that representation in mathematical ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(documents):\n",
    "    tokenized_documents = [word_tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = augmented_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we've got a way of performing tfidf, let's run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4729550745276567, 1.4729550745276567, 2.209432611791485, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.689572226371526, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 2.252762968495368, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.689572226371526, 1.4729550745276567, 1.4729550745276567, 1.126381484247684, 1.3854733952904028, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.126381484247684, 1.4729550745276567, 1.126381484247684, 1.4729550745276567, 1.4729550745276567, 1.0023541774659097, 1.4729550745276567, 1.4729550745276567, 1.126381484247684, 1.126381484247684, 1.4729550745276567, 2.209432611791485, 1.4729550745276567, 2.209432611791485, 1.126381484247684, 1.4729550745276567, 2.209432611791485, 1.126381484247684, 1.4729550745276567, 1.4729550745276567, 1.689572226371526, 1.4729550745276567, 2.209432611791485, 1.126381484247684, 1.4729550745276567, 0.9236489301936018, 1.126381484247684, 2.209432611791485, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.126381484247684, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.126381484247684, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.3854733952904028, 1.4729550745276567, 2.209432611791485, 1.4729550745276567, 1.4729550745276567, 1.126381484247684, 1.4729550745276567, 1.126381484247684, 2.209432611791485, 1.126381484247684, 1.4729550745276567, 1.4729550745276567, 0.9236489301936018, 1.4729550745276567, 2.209432611791485, 2.209432611791485, 1.4729550745276567, 0.7798078939677113, 2.209432611791485, 1.126381484247684, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.4729550745276567, 1.1541506798272585, 2.209432611791485, 1.126381484247684, 1.4729550745276567, 1.4729550745276567, 1.126381484247684, 1.4729550745276567, 2.209432611791485, 1.126381484247684, 1.4729550745276567] China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\n"
     ]
    }
   ],
   "source": [
    "tfidf_representation = tfidf(all_documents)\n",
    "print(tfidf_representation[0], document_0)\n",
    "# doc vector and document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": true
  },
  "toc_position": {
   "height": "206px",
   "left": "596.375px",
   "right": "20px",
   "top": "167px",
   "width": "302px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
