{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF on Scikit-Learn\n",
    "### During the last class we implemented TF-IDF by hand. While it's been a good exercise there are packages that implement this much more quickly - like Scikit-Learn.\n",
    "\n",
    "### Let's take a look at how we might code this in Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.31019096605521496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18320378146489946, 0.0, 0.18320378146489946, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23957330918096045, 0.0, 0.0, 0.0, 0.18320378146489946, 0.0, 0.23957330918096045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23957330918096045, 0.0, 0.23957330918096045, 0.23957330918096045, 0.0, 0.15022972156764192, 0.0, 0.0, 0.0, 0.0, 0.23957330918096045, 0.0, 0.0, 0.15022972156764192, 0.23957330918096045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23957330918096045, 0.0, 0.23957330918096045, 0.0, 0.0, 0.0, 0.23957330918096045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23957330918096045, 0.23957330918096045, 0.10868731908150663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23957330918096045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    "\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "document_0 = \"China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\"\n",
    "document_1 = \"At last, China seems serious about confronting an endemic problem: domestic violence and corruption.\"\n",
    "document_2 = \"Japan's prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.\"\n",
    "document_3 = \"Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.\"\n",
    "document_4 = \"What's the future of Abenomics? We asked Shinzo Abe for his views\"\n",
    "document_5 = \"Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble's value falls almost daily.\"\n",
    "document_6 = \"Vladimir Putin was found to be riding a horse, again, without a shirt on while hunting deer. Vladimir Putin always seems so serious about things - even riding horses.\"\n",
    "\n",
    "all_documents = [document_0, document_1, document_2, document_3, document_4, document_5, document_6]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents] # tokenized docs\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(all_documents)\n",
    "print(sklearn_representation.toarray()[0].tolist())\n",
    "print(document_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "### Now that we've covered TF-IDF and how to do with our own code as well as Scikit-Learn. Let's take a look at how we can actually compare different documents with cosine similarity or the Euclidean dot product formula.\n",
    "\n",
    "### At this point our documents are represented as vectors.\n",
    "\n",
    "### With cosine similarity we can measure the similarity between two document vectors. I'm not going to delve into the mathematical details about how this works but basically we turn each document into a line going from point X to point Y. We then compare that directionality with the second document into a line going from point V to point W. We measure how large the cosine angle is in between those representations. \n",
    "\n",
    "<img src=cosine1.png>\n",
    "\n",
    "### Now in our case, if the cosine similarity is 1, they are the same document. If it is 0, the documents share nothing. This is because term frequency cannot be negative so the angle between the two vectors cannot be greater than 90°.\n",
    "\n",
    "### Here's our representation of cosine similarity of two vectors in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Digression: how the zip function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (2, 5), (3, 6)]\n",
      "4\n",
      "10\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "zipped = zip(x, y)\n",
    "print(list(zipped))\n",
    "for p,q in zip(x, y):\n",
    "    print(p*q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a vector representation and a way to compare different vectors we can put it to good use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.081143050524630528"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(sklearn_representation.toarray()[0],sklearn_representation.toarray()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Corpora\n",
    "\n",
    "### Now that we learned how to use several NLTK functions, we create a pratical example of how to explore one of the corpora.\n",
    "\n",
    "### Task: Use the NLTK corpus module to read the corpus austen-persuasion.txt, included in the Gutenberg corpus collection, and answer the following questions:\n",
    "\n",
    "### - How many total words does this corpus have ?\n",
    "\n",
    "### - How many unique words does this corpus have ?\n",
    "\n",
    "### - What are the counts for the 10 most frequent words ?\n",
    "\n",
    "### Besides the corpus module that allows us to access and explore the bundled corpora with ease, NLTK also provides the probability module that contains several useful classes and functions for the task of computing probability distributions. One such class is called FreqDist and it keeps track of the sample frequencies in a distribution.\n",
    "\n",
    "### The next set of cells show how to use these two modules to perform the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "# first, import the gutenberg collection\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# what corpora are in the collection ?\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import FreqDist class\n",
    "from nltk import FreqDist\n",
    "\n",
    "# create frequency distribution object\n",
    "fd = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 6132 samples and 98171 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# for each token in the relevant text, increment its counter\n",
    "for word in gutenberg.words('austen-persuasion.txt'):\n",
    "    fd[word] += 1\n",
    "    \n",
    "print(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98171\n"
     ]
    }
   ],
   "source": [
    "print(fd.N()) # total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6132\n"
     ]
    }
   ],
   "source": [
    "print(fd.B())# number of bins or unique samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 6750\n",
      "the 3120\n",
      "to 2775\n",
      ". 2741\n",
      "and 2739\n",
      "of 2564\n",
      "a 1529\n",
      "in 1346\n",
      "was 1330\n",
      "; 1290\n"
     ]
    }
   ],
   "source": [
    "# Get a list of the top 10 words sorted by frequency\n",
    "for word, count in fd.most_common(10):\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Solution**: Jane Austen’s book Persuasion contains 98171 total tokens and 6132 unique tokens. Out of these, the most common token is a comma, followed by the word the. In fact, the last part of this task is the perfect segue for one of the most interesting empirical observations about word occurrences. If we were to take a large corpus, count up the number of times each word occurs in that corpus and then list the words according to the number of occurrences (starting with the most frequent), we would be able to observe a direct relationship between the frequency of a word and its position in the list. In fact, Zipf claimed this relationship could be expressed mathematically, i.e., for any given word, fr = k, where f is the frequency of that word, r is the rank, or the position of the word in the sorted list, and k is a constant. So, for example, the 5th most frequent word should occur exactly two times as frequently as the 10th most frequent word. In NLP literature, the above relationship is usually referred to as Zipf’s Law.\n",
    "\n",
    "###### Even though the mathematical relationship prescribed by Zipf’s Law may not hold exactly, it is useful to describe how words are distributed in human languages - there are a few words that are very common, a few that occur with medium frequency and a very large number of words that occur very rarely. It’s simple to extend the last part of our task and graphically visualize this relationship using NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAESCAYAAADuVeJ5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115239ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count each token in each text of the Gutenberg collection\n",
    "fd = FreqDist()\n",
    "for text in gutenberg.fileids():\n",
    "    for word in gutenberg.words(text):\n",
    "        fd[word] += 1\n",
    "\n",
    "# Initialize two empty lists which will hold our ranks and frequencies\n",
    "ranks = []\n",
    "freqs = []\n",
    "\n",
    "# Generate a (rank, frequency) point for each counted token and \n",
    "# append to the respective lists. Note that the iteration\n",
    "# over fd is automatically sorted.\n",
    "for rank, (word, value) in enumerate(fd.most_common()):\n",
    "    ranks.append(rank + 1)\n",
    "    freqs.append(fd[word])\n",
    "\n",
    "# Plot rank vs frequency on a log-log plot and show the plot\n",
    "plt.loglog(ranks, freqs)\n",
    "plt.xlabel('frequency(f)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('rank(r)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Words\n",
    "\n",
    "### Now that we have learnt how to explore a corpus, let’s define a task that can put such explorations to use.\n",
    "\n",
    "### *Task*: Train and build a word predictor, i.e., given a training corpus, write a program that can predict the word that follows a given word. Use this predictor to generate a random sentence of 20 words.\n",
    "\n",
    "### To build a word predictor, we first need to compute a distribution of two-word sequences over a training corpus, i.e., we need to keep count the occurrences of a word given the previous word as a context for that word. Once we have computed such a distribution, we can use the input word to find a list of all possible words that followed it in the training corpus and then output a word at random from this list. To generate a random sentence of 20 words, all we have to do is to start at the given word, predict the next word using this predictor, then the next and so on until we get a total of 20 words. The next set of cells illustrate how to accomplish this easily using the modules provided by NLTK. We use Jane Austen’s Persuasion as the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ConditionalFreqDist \n",
    "from random import choice\n",
    "\n",
    "# Create conditional distribution object\n",
    "cfd = ConditionalFreqDist() # we create an object out of class ConditionalFreqDist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see http://www.nltk.org/_modules/nltk/probability.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foul air than not think herself would think more exquisitely happy enough by side of furniture of illness of Henrietta "
     ]
    }
   ],
   "source": [
    "# For each token, count current word given previous word\n",
    "prev_word = None\n",
    "for word in gutenberg.words('austen-persuasion.txt'):\n",
    "    cfd[prev_word][word] += 1\n",
    "    prev_word = word\n",
    "\n",
    "# Start predicting at the given word, say ’therefore’\n",
    "word = 'foul'\n",
    "i = 1\n",
    "\n",
    "# Find all words that can possibly follow the current word and choose one at random\n",
    "while i < 21: # keeps count, once we arrive at 20 words we end the loop\n",
    "    print(word, end=\" \")\n",
    "    lwords = list(cfd[word].keys())\n",
    "    follower = choice(lwords)\n",
    "    word = follower\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Solution**: The 20 word output sentence is, of course, not grammatical but every two word sequence will be because the training corpus that we used for estimating our conditional frequency distribution is grammatical and because of the way that we estimated the conditional frequency distribu- tion. Note that for our task we used only the previous word as the context for our predictions. It is certainly possible to use the previous two or, even, three words as the prediction context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering Part-Of-Speech Tags\n",
    "\n",
    "### NLTK comes with an excellent set of modules to allow us to train and build relatively sophisticated POS taggers. However, for this task, we will restrict ourselves to a simple analysis on an already tagged corpus included with NLTK.\n",
    "\n",
    "### **Task***: Tokenize the included Brown Corpus and build one or more suitable data structures so that you can answer the following questions:\n",
    "\n",
    "### - What is the most frequent tag?\n",
    "\n",
    "### - Which word has the most number of distinct tags?\n",
    "\n",
    "### - What is the ratio of masculine to feminine pronouns?\n",
    "\n",
    "### - How many words are ambiguous, in the sense that they appear with at least two tags ?\n",
    "\n",
    "### For this task, it is important to note that there is are two versions of the Brown corpus that comes bundled with NLTK: the first is the raw corpus that we used in the last two tasks, and the second is a tagged version wherein each token of each sentence of the corpus has been annotated with the correct POS tags. Each sentence in this version of a corpus is represented as a list of 2-tuples, each of the form (token, tag). For example, a sentence like “the ball is green”, from a tagged corpus, will be represented inside NLTK as the list [('the','at'), ('ball','nn'), ('is',’vbz'), ('green','jj')] .\n",
    "\n",
    "### As explained before, the Brown corpus comprises of 15 different sections, represented by the letters 'a' through 'r'. Each of the sections represents a different genre of text and for certain NLP tasks not discussed in this article, this division proves very useful. Given this information, all we should have to do is build the data structures to analyze this tagged corpus. Looking at the kinds of questions that we need to answer, it will be sufficient to build a frequency distribution over the POS tags and a conditional frequency distribution over the tags using the tokens as the context. The next set of cells illustrate the solution for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk import FreqDist, ConditionalFreqDist\n",
    "fd = FreqDist() # we create an object of the class FreqDist\n",
    "cfd = ConditionalFreqDist()\n",
    "\n",
    "# for each tagged sentence in the corpus, get the (token, tag) pair and update\n",
    "# both count(tag) and count(tag given token)\n",
    "for sentence in brown.tagged_sents(): # tagged_sents is a fxn that already displays it as sentence, then gives a tag for each word in the sentence\n",
    "    for (token, tag) in sentence: \n",
    "        fd[tag] += 1 # get the count of all tags in the sentence\n",
    "        cfd[token][tag] += 1 # get count of how many times the word is tagged (ex. token = country, tag= 'nn', it counts how may times that the country was tagged as noun in the corpus)\n",
    "\n",
    "# The most frequent tag is ...\n",
    "fd.max() # search what is the max tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 'that')\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to hold (numtags,word) tuple\n",
    "wordbins = [] # bins holds number of frequency of the token/word, and word  \n",
    "\n",
    "# append each (n(unique tags for token),token) tuple to list\n",
    "for token in cfd.conditions(): # conditions allows you to loop over dictionary within dictionary\n",
    "    wordbins.append((cfd[token].B(), token)) \n",
    "\n",
    "# sort tuples by number of unique tags (highest first)\n",
    "wordbins.sort(reverse=True)\n",
    "\n",
    "# the token with the maximum number of possible part-of-speech tags is ...\n",
    "print(wordbins[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2576884422110552\n"
     ]
    }
   ],
   "source": [
    "# masculine pronouns\n",
    "male = ['he', 'his', 'him', 'himself']\n",
    "\n",
    "# feminine pronouns\n",
    "female = ['she', 'hers', 'her', 'herself']\n",
    "\n",
    "# initialize counters\n",
    "n_male, n_female = 0, 0\n",
    "\n",
    "# total number of masculine samples\n",
    "for m in male:\n",
    "    n_male += cfd[m].N() # we get the pronoun in male list, and show the number of times this appears\n",
    "\n",
    "# total number of feminine samples\n",
    "for f in female:\n",
    "    n_female += cfd[f].N() # we get the pronoun in female list, and show the number of times this appears\n",
    "\n",
    "# calculate required ratio\n",
    "print(float(n_male)/n_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8729\n"
     ]
    }
   ],
   "source": [
    "n_ambiguous = 0\n",
    "for (ntags, token) in wordbins:\n",
    "    if ntags > 1: \n",
    "        n_ambiguous += 1\n",
    "\n",
    "# number of tokens with more than a single POS tag\n",
    "print(n_ambiguous) # shows the number of words with more than one tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Solution**: The most frequent POS tag in the Brown corpus is, unsurprisingly, the noun (NN). The word that has the most number of unique tags is, in fact, the word that. There are almost 3 times as many masculine pronouns in the corpus as feminine pronouns and, finally, there are as many as 8700 words in the corpus that can be deemed ambiguous - a number that should indicate the difficulty of the POS-tagging task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Association\n",
    "\n",
    "### The task of free word association is a very common one when it comes to psycholinguistics, especially in the context of lexical retrieval -- human subjects respond more readily to a word if it follows another highly associated word as opposed to a completely unrelated word. The instructions for performing the association are fairly straightforward -- the subject is asked for the word that immediately comes to mind upon hearing a particular word.\n",
    "\n",
    "### **Task**: Use a large POS-tagged text corpus to perform free word association. You may ignore function words and assume that the words to be associated are always nouns.\n",
    "\n",
    "### For this task, we will use the concept of word co-occurrences, i.e., counting the number of times words occur in close proximity with each other and then using these counts to estimate the degree of association. For each token in each sentence, we will look at all following tokens that lie within a fixed window and count their occurrences in this context using a con- ditional frequency distribution. The next set of cells show how we accomplish this using Python and NLTK with a window size of 5 and the POS-tagged version of the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, stopwords\n",
    "\n",
    "# initialize a new conditional distribution\n",
    "cfd = ConditionalFreqDist()\n",
    "\n",
    "# get a list of English stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "def is_noun(tag): # we define a noun; noun is tagged as those below in red\n",
    "    return tag.lower() in ['nn','nns','nn$','nn-tl','nn+bez', 'nn+hvz', \n",
    "                           'nns$','np','np$','np+bez','nps', 'nps$','nr',\n",
    "                           'np-tl','nrs','nr$']\n",
    "\n",
    "for sentence in brown.tagged_sents(): #\n",
    "    for (index, tagtuple) in enumerate(sentence):\n",
    "        (token, tag) = tagtuple # we take the tagtuple and split into the word (token) and then the tag\n",
    "        token = token.lower()\n",
    "        if token not in stopwords_list and is_noun(tag): # if the word is not in the stopword and it is a noun\n",
    "            window = sentence[index+1:index+5] #we take the 5 words that come after it \n",
    "            for (window_token, window_tag) in window: # then the word in window\n",
    "                window_token = window_token.lower() # we take the word in window to lower case\n",
    "                if window_token not in stopwords_list and is_noun(window_tag): # then check if this word is not in the stopword and is a noun \n",
    "                    cfd[token][window_token] += 1 # then  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "god\n"
     ]
    }
   ],
   "source": [
    "# OK. We are done ! Let's start associating !\n",
    "print(cfd['america'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notes\n"
     ]
    }
   ],
   "source": [
    "print(cfd['franc'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman\n"
     ]
    }
   ],
   "source": [
    "print(cfd['man'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "print(cfd['woman'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "girl\n"
     ]
    }
   ],
   "source": [
    "print(cfd['boy'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyes\n"
     ]
    }
   ],
   "source": [
    "print(cfd['girl'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female\n"
     ]
    }
   ],
   "source": [
    "print(cfd['male'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player\n"
     ]
    }
   ],
   "source": [
    "print(cfd['ball'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bills\n"
     ]
    }
   ],
   "source": [
    "print(cfd['doctor'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block\n"
     ]
    }
   ],
   "source": [
    "print(cfd['road'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The “word associator” that we have built seems to work surprisingly well, especially when compared to the minimal amount of effort that was required. (In fact, in the context of folk psychology, our associator would almost seem to have a personality, albeit a pessimistic and misogynistic one). The results of this task should be a clear indication of the usefulness of corpus linguistics in general. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
